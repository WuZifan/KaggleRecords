{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow回忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们搭建了一个没有隐层，只有一层输入和一层输出的人工神经网络（ANN）。\n",
    "\n",
    "我们可以简单回忆一下搭建这个神经网络的步骤：\n",
    "\n",
    "* 1）确定结构：一层输入，一层输出，没有隐层；输入层节点数等于特征数目，输出层节点数等于label种类数。\n",
    "\n",
    "* 2）确定激活函数：softmax\n",
    "\n",
    "* 3）确定损失函数：log损失函数\n",
    "\n",
    "* 4）确定优化方法：梯度下降方法；实际训练有随机梯度下降（batch-gradient-descent）\n",
    "\n",
    "那么在实际搭建中，tensorflow（tf）网络是这么建立的：\n",
    "\n",
    "* 1）训练样本x,y：两个tf的placeholder，因为x,y作为训练样本需要不断传入新的，因此用place-holder。\n",
    "\n",
    "* 2）参数w,b：w和b分别表示weights和bias，用tf的variable来进行表示，他们不用随时传递新的值，只需要不断更新就好。\n",
    "\n",
    "* 3）输出Result：利用tf的乘法和简单加法计算y=w*x+b，得到每个节点的线性结果。\n",
    "\n",
    "* 4）激活结果pred：prediction通过对输出result运用tf自带的softmax函数得到。\n",
    "\n",
    "* 5）损失函数loss：通过tf自带的函数得到，并以真实值y和预测值pred作为输入。\n",
    "\n",
    "* 6）优化函数train：通过tf自带的梯度下降函数得到，并将loss作为参数传入。\n",
    "\n",
    "* 7）运行时，利用sess运行train，并把由实际数据分割成的batch_x和batch_y喂给x和y。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "昨天的训练结果比较差，只能在90%左右徘徊，今天通过搭建一个CNN，来将结果尽可能提高到98%。\n",
    "\n",
    "关于CNN的相关知识，可以回去查笔记。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载包和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('input/train.csv',index_col=False)\n",
    "test=pd.read_csv('input/test.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取特征数据&预处理\n",
    "\n",
    "这里我们提取train数据中的特征数据x，并做简单的预处理（归一化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train.iloc[:,1:].values\n",
    "x_train=x_train.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**归一化处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.multiply(x_train,1.0/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**计算图片长和高**\n",
    "\n",
    "CNN会用到图片的长和高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size=x_train.shape[1]\n",
    "image_width=image_height=np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "image_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取label数据和预处理\n",
    "\n",
    "这里我们把样本的label数据提取出来，并做one-hot编码，方便训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得一共有几个label\n",
    "labels_count=np.unique(train.iloc[:,0]).shape[0]\n",
    "labels_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**利用pd进行one-hot编码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.get_dummies(train.iloc[:,0]).values\n",
    "labels[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分训练集和验证集\n",
    "\n",
    "我们对大的训练集做简单划分，划分成训练集和验证集，从而让我们的模型可以在验证集上跑一跑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE=2000\n",
    "\n",
    "train_images=x_train[VALIDATION_SIZE:]\n",
    "train_labels=labels[VALIDATION_SIZE:]\n",
    "\n",
    "validation_images=x_train[:VALIDATION_SIZE]\n",
    "validation_labels=labels[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**批处理**\n",
    "\n",
    "由于数据量比较大，一次吃这么多数据进去训练慢，而且内存压力大。\n",
    "\n",
    "因此我们采用批处理，让训练数据分批训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "n_batch=int(len(train_images)/batch_size)\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型\n",
    "\n",
    "模型搭建主要需要有这么几个内容：\n",
    "\n",
    "    1、用来接收特征和样本的两个placeholder\n",
    "    \n",
    "    2、若干个表示weights和bias的variable。\n",
    "    \n",
    "    3、将weights和variable与特征x组合起来的计算节点。\n",
    "    \n",
    "    4、将计算节点的输出进行激活的激活函数节点。\n",
    "    \n",
    "    5、以及根据预测值和真实值定义的损失函数。\n",
    "    \n",
    "    6、最后是根据损失函数定义的优化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本和label的接收节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的None表示x和y传入几个样本都没关系，只要样本的大小是784或者10就可以。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights和bias初始化方法\n",
    "\n",
    "由于这次的神经网络层比较多，我们逐层手动写会比较麻烦，这里我们通过构建两个方法来实现weights和bias的初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    # 表示初始化权重为正态分布，标准差为0.1\n",
    "    initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # 初始化bias为常量0.1\n",
    "    initial=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层和池化层\n",
    "\n",
    "卷积层和池化层可以看做是对一个矩阵的特殊操作，具体的操作这里不多提。\n",
    "\n",
    "那么为了不重复手工定义这个操作，我们同样用函数来定义一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入张量化\n",
    "\n",
    "呃，我也不知道这个是啥= - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image=tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义神经网络结构\n",
    "\n",
    "那么现在开始，我们就要来搭建自己的神经网络啦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义第一层的weights\n",
    "W_conv1=weight_variable([3,3,1,32])\n",
    "\n",
    "# 定义第一层的bias\n",
    "b_conv1=bias_variable([32])\n",
    "\n",
    "# 对第一层数据做卷积\n",
    "h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "\n",
    "# 对卷积结果做池化\n",
    "h_pool1=max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备第二层参数\n",
    "W_conv2=weight_variable([6,6,32,64])\n",
    "b_conv2=bias_variable([64])\n",
    "\n",
    "# 对第二层数据做卷积\n",
    "h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "\n",
    "# 对第二层做池化\n",
    "h_pool2=max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**全连接层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展开第二层池化结果\n",
    "h_pool2_flat=tf.reshape(h_pool2,[-1,7*7*64])\n",
    "\n",
    "# 定义全连接层参数\n",
    "W_fc1=weight_variable([7*7*64,1024])\n",
    "b_fc1=bias_variable([1024])\n",
    "\n",
    "# 激活全连接层\n",
    "h_fc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对全连接层做dropout**\n",
    "\n",
    "全连接层用1024个节点，太多了，我们需要drop_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob=tf.placeholder(tf.float32)\n",
    "h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**输出层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2=weight_variable([1024,10])\n",
    "b_fc2=bias_variable([10])\n",
    "y_conv=tf.matmul(h_fc1_drop,W_fc2)+b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数和优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_conv))\n",
    "\n",
    "# 梯度下降优化\n",
    "train_step_1=tf.train.AdadeltaOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义精度计算节点以及保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**精度计算方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义在验证集上计算精度的节点\n",
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_conv,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**保存模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练\n",
    "\n",
    "最激动人心的时刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2轮，准确度为：0.896\n",
      "第3轮，准确度为：0.9275\n",
      "第4轮，准确度为：0.946\n",
      "第5轮，准确度为：0.951\n",
      "第6轮，准确度为：0.955\n",
      "第7轮，准确度为：0.962\n",
      "第8轮，准确度为：0.9655\n",
      "第9轮，准确度为：0.968\n",
      "第10轮，准确度为：0.9735\n",
      "第11轮，准确度为：0.976\n",
      "第12轮，准确度为：0.9765\n",
      "第13轮，准确度为：0.978\n",
      "第14轮，准确度为：0.9785\n",
      "第15轮，准确度为：0.984\n",
      "第16轮，准确度为：0.9825\n",
      "第17轮，准确度为：0.982\n",
      "第18轮，准确度为：0.985\n",
      "第19轮，准确度为：0.986\n",
      "第20轮，准确度为：0.985\n",
      "第21轮，准确度为：0.9855\n",
      "第22轮，准确度为：0.987\n",
      "第23轮，准确度为：0.986\n",
      "第24轮，准确度为：0.9875\n",
      "第25轮，准确度为：0.988\n",
      "第26轮，准确度为：0.989\n",
      "第27轮，准确度为：0.9885\n",
      "第28轮，准确度为：0.9895\n",
      "第29轮，准确度为：0.9895\n",
      "第30轮，准确度为：0.9885\n"
     ]
    }
   ],
   "source": [
    "# 别忘了初始化参数\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(1,30):\n",
    "        for batch in range(n_batch):\n",
    "            # 得到批数据，利用批数据进行训练\n",
    "            batch_x=train_images[(batch)*batch_size:(batch+1)*batch_size]\n",
    "            batch_y=train_labels[(batch)*batch_size:(batch+1)*batch_size]\n",
    "            # 运行整个模型\n",
    "            sess.run(train_step_1,feed_dict={x:batch_x,y:batch_y,keep_prob:0.5})\n",
    "            \n",
    "        # 每个周期算一个精度\n",
    "        accuracy_n=sess.run(accuracy,feed_dict={x:validation_images,y:validation_labels,keep_prob:1.0})\n",
    "        \n",
    "        # 打印结果\n",
    "        print(\"第\"+str(epoch+1)+\"轮，准确度为：\"+str(accuracy_n))\n",
    "        \n",
    "        # 保存训练模型\n",
    "        global_step.assign(epoch).eval()\n",
    "        saver.save(sess,\"save/model.ckpt\",global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "接下来，就可以用我们训练好的模型开始预测了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model.ckpt-29\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess,'save/model.ckpt-29')\n",
    "    \n",
    "    test_x=np.array(test,dtype=np.float32)\n",
    "    \n",
    "    conv_y_predict=y_conv.eval(feed_dict={x:test_x[1:100,:],keep_prob:1.0})\n",
    "    \n",
    "    conv_y_predict_all=list()\n",
    "    \n",
    "    for i in np.arange(100,28001,100):\n",
    "        \n",
    "        conv_y_predict=y_conv.eval(feed_dict={x:test_x[i-100:i,:],keep_prob:1.0})\n",
    "        \n",
    "        test_pred=np.argmax(conv_y_predict,axis=1)\n",
    "        \n",
    "        conv_y_predict_all=np.append(conv_y_predict_all,test_pred)\n",
    "    \n",
    "    submission=pd.DataFrame(data={'ImageId':range(1,28001),'Label':np.int32(conv_y_predict_all)})\n",
    "    \n",
    "    submission.to_csv('Digit1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
